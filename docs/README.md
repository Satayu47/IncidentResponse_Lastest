# Incident Response Platform

**Combined Phase-1 + Phase-2 Incident Response System**

Phase-1 provides LLM-based incident classification with a ChatOps interface.  
Phase-2 delivers DAG-based playbook execution and automation engine.

---

## ğŸ—ï¸ Project Structure

```
IncidentResponse_Combine/
â”œâ”€â”€ app.py                              # Streamlit UI (Phase-1 + Phase-2)
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ .env.example                        # Environment template
â”œâ”€â”€ src/                                # Phase-1 modules
â”‚   â”œâ”€â”€ llm_adapter.py                 # OpenAI API wrapper
â”‚   â”œâ”€â”€ extractor.py                   # IOC extraction
â”‚   â”œâ”€â”€ dialogue_state.py              # Multi-turn conversation
â”‚   â”œâ”€â”€ explicit_detector.py           # Keyword-based detection
â”‚   â”œâ”€â”€ classification_rules.py        # OWASP normalization
â”‚   â”œâ”€â”€ nvd.py                         # NVD API client
â”‚   â”œâ”€â”€ lc_retriever.py                # Knowledge base retriever
â”‚   â””â”€â”€ owasp_display.py               # UI formatting
â”œâ”€â”€ phase2_engine/                      # Phase-2 automation
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ runner.py                  # Playbook executor
â”‚   â”‚   â”œâ”€â”€ runner_bridge.py           # Phase-1 â†’ Phase-2 glue
â”‚   â”‚   â”œâ”€â”€ playbook_loader.py         # YAML playbook loader
â”‚   â”‚   â”œâ”€â”€ playbook_dag.py            # DAG construction
â”‚   â”‚   â”œâ”€â”€ automation.py              # Automation actions
â”‚   â”‚   â””â”€â”€ policy.py                  # Policy enforcement
â”‚   â””â”€â”€ playbooks/                      # OWASP playbooks
â”‚       â”œâ”€â”€ A01_broken_access_control.yaml
â”‚       â”œâ”€â”€ A02_cryptographic_failures.yaml
â”‚       â”œâ”€â”€ A03_injection.yaml
â”‚       â”œâ”€â”€ A04_insecure_design.yaml
â”‚       â”œâ”€â”€ A05_misconfiguration.yaml
â”‚       â”œâ”€â”€ A06_vulnerable_components.yaml
â”‚       â”œâ”€â”€ A07_authentication_failures.yaml
â”‚       â””â”€â”€ A10_ssrf.yaml
â””â”€â”€ tests/                              # Test suite
    â”œâ”€â”€ test_phase1_classification.py
    â”œâ”€â”€ test_phase2_automation.py
    â”œâ”€â”€ test_human_multiturn_full.py   # 100-case human-style test suite
    â””â”€â”€ generate_accuracy_report.py    # Accuracy report generator
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```powershell
pip install -r requirements.txt
```

### 2. Configure Environment

Copy `.env.example` to `.env` and add your OpenAI API key:

```powershell
Copy-Item .env.example .env
```

Edit `.env`:
```
OPENAI_API_KEY=sk-your-actual-key-here
```

### 3. Run the Application

```powershell
streamlit run app.py
```

The application will open in your browser at `http://localhost:8501`

---

## ğŸ“– Usage

### Phase-1: Incident Classification

1. **Describe the incident** in the text area
   - Include IPs, URLs, attack patterns, CVEs
   - Be as detailed as possible

2. **Click "Classify Incident"**
   - System extracts IOCs (IPs, URLs, CVEs)
   - LLM classifies into OWASP Top 10 category
   - Confidence score is calculated

3. **Review classification results**
   - See incident type and confidence
   - View extracted indicators
   - Check OWASP category details

### Phase-2: Automated Response

Once confidence reaches 65% or higher:

1. **Click "Generate Response Plan"**
   - System maps incident â†’ playbooks
   - Builds execution DAG
   - Generates step-by-step plan

2. **Review the response plan**
   - Steps grouped by NIST IR phases:
     - ğŸ›¡ï¸ Preparation
     - ğŸ” Detection & Analysis
     - âš ï¸ Containment
     - ğŸ§¹ Eradication
     - â™»ï¸ Recovery
     - ğŸ“‹ Post-Incident Review

3. **Execute automation** (optional)
   - Uncheck "Dry Run" to execute real actions
   - **âš ï¸ Use with extreme caution in production!**

---

## ğŸ”§ Configuration

### Confidence Threshold

Adjust in `.env`:
```
CONFIDENCE_THRESHOLD=0.65
```

Or modify `THRESH_GO` in `app.py`.

### LLM Model Selection

Choose model in the sidebar:
- `gpt-4o-mini` (fast, cost-effective)
- `gpt-4o` (balanced)
- `gpt-4-turbo` (most capable)

### Automation Settings

Phase-2 automation is disabled by default (dry-run mode).

To enable real automation:
1. Set `ENABLE_AUTOMATION=true` in `.env`
2. Uncheck "Dry Run" in the UI
3. **âš ï¸ Test thoroughly in a sandbox environment first!**

---

## ğŸ¯ How It Works

### Phase-1 Flow

```
User Description
    â†“
IOC Extraction (IPs, URLs, CVEs)
    â†“
LLM Classification â†’ OWASP Category
    â†“
Confidence Check (â‰¥ 65%?)
    â†“
Ready for Phase-2
```

### Phase-2 Flow

```
Incident Type
    â†“
Map to Playbook IDs
    â†“
Load YAML Playbooks
    â†“
Build DAG (topological order)
    â†“
Policy Validation
    â†“
Execute Actions (dry-run or real)
    â†“
Response Plan Display
```

### Incident â†’ Playbook Mapping

Configured in `phase2_engine/core/runner_bridge.py`:

```python
INCIDENT_TO_PLAYBOOK = {
    "sql_injection": ["A03_injection"],
    "xss": ["A03_injection"],
    "broken_access_control": ["A01_broken_access_control"],
    "cryptographic_failures": ["A02_cryptographic_failures"],
    # ... etc
}
```

---

## ğŸ§ª Testing

Run tests:

```powershell
pytest tests/
```

With coverage:

```powershell
pytest --cov=src --cov=phase2_engine tests/
```

---

## ğŸ›¡ï¸ Security Considerations

### Phase-1
- API keys stored in `.env` (never commit!)
- Input sanitization for all user text
- Rate limiting on LLM calls

### Phase-2
- **Dry-run default:** No destructive actions by default
- **Policy engine:** Validates actions before execution
- **Approval levels:** High-risk actions require approval
- **Audit logging:** All actions are logged

### âš ï¸ Production Deployment

**DO NOT** run Phase-2 automation in production without:
1. Thorough testing in sandbox
2. Policy configuration review
3. Approval workflows
4. Comprehensive logging/monitoring
5. Rollback procedures

---

## ğŸ“š Playbook Structure

Playbooks are YAML files in `phase2_engine/playbooks/`:

```yaml
id: A03_injection
name: Injection Attack Response
description: Response playbook for injection attacks
severity: critical

phases:
  preparation:
    - action: verify_waf_enabled
      name: Verify WAF Enabled
      automated: false
  
  detection_analysis:
    - action: analyze_injection_payload
      name: Analyze Injection Payload
      automated: true
      params:
        log_sources: ["waf", "application"]
  
  containment:
    - action: enable_waf_rule
      name: Enable WAF Rule
      automated: true
  # ... more phases
```

---

## ğŸ”„ Multi-Incident Merging

To merge multiple incidents:

1. Classify the primary incident
2. Add additional incidents in "Additional incidents to merge" text area
3. Click "Generate Response Plan"
4. System will merge playbooks using DAG composition

---

## ğŸ¤ Contributing

This is an educational/demo project combining two approaches:
- **Your repo:** Phase-1 ChatOps + classification
- **Friend's repo (IR-SANDBOX):** Phase-2 playbook + DAG

To extend:
1. Add new playbooks in `phase2_engine/playbooks/`
2. Update mapping in `runner_bridge.py`
3. Add automation actions in `automation.py`
4. Configure policies in `policy.py`

---

## ğŸ§ª Testing & Accuracy

### 100-Case Human-Style Test Suite

The project includes comprehensive testing with **100 real-world test cases**:

- **72 single-incident tests:** Human-style multi-turn conversations for accuracy metrics
- **28 multi-incident tests:** Complex scenarios for playbook merging and DAG validation

**Run tests:**
```powershell
# Install test dependencies
pip install pytest pytest-json-report

# Run full test suite
pytest tests/test_human_multiturn_full.py -v

# Generate accuracy report
pytest tests/test_human_multiturn_full.py -v --json-report --json-report-file=tests/results.json
python tests/generate_accuracy_report.py tests/results.json

# View results
cat tests/ACCURACY_REPORT.md
```

### Multilabel DAG Merge Tests

Validates Phase-2 playbook merging for multi-vector incidents:

- **22 merge scenarios:** 2-label, 3-label, and stress tests
- **All 8 OWASP playbooks:** Verified individually and in combinations
- **Critical four validation:** A01, A04, A05, A07 merge tested

**Run merge tests:**
```powershell
# Run DAG merge validation
pytest tests/test_multilabel_merge.py -v

# View merge report
cat tests/MULTILABEL_MERGE_REPORT.md
```

**Test categories:**
- Broken Access Control (12 tests)
- Injection (12 tests)
- Broken Authentication (12 tests)
- Security Misconfiguration (12 tests)
- Sensitive Data Exposure (8 tests)
- Cryptographic Failures (8 tests)
- Other/Non-Security (8 tests)

See **[RUN_TESTS.md](RUN_TESTS.md)** for detailed testing guide.

---

## ğŸ“„ License

MIT License - Educational purposes

---

## ğŸ™ Acknowledgments

- Phase-1: LLM-first incident classification
- Phase-2: DAG-based automation engine (IR-SANDBOX concept)
- OWASP Top 10 2021
- NIST Incident Response Framework

---

## ğŸ“ Support

For issues or questions, create a GitHub issue or refer to the documentation in each module's docstrings.

---

**Built with â¤ï¸ for security teams everywhere**
