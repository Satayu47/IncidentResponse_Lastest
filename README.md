# Incident Response ChatOps Assistant

Automated incident response system using LLM-based classification and playbook generation.

## ğŸ¯ Key Results
- **98.0% Classification Accuracy** (49/50 hard test cases)
- **100% Playbook Validation** (28/28 multi-playbook tests)
- **Hybrid Approach**: Rule-based + LLM + Canonical Mapping
- **Baseline Comparison**: Supports Gemini, OpenAI, and Claude models

## ğŸ“ Project Structure

```
incidentResponse_Combine/
â”œâ”€â”€ app.py                      # Main Streamlit web application
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup.ps1                   # Quick setup script
â”‚
â”œâ”€â”€ src/                        # Core source code
â”‚   â”œâ”€â”€ phase1_core.py         # Classification pipeline
â”‚   â”œâ”€â”€ llm_adapter.py         # Multi-LLM integration (Gemini, OpenAI, Claude)
â”‚   â”œâ”€â”€ explicit_detector.py   # Regex pattern detection (100+ patterns)
â”‚   â”œâ”€â”€ classification_rules.py # Canonical label mapping (90+ variations)
â”‚   â””â”€â”€ playbook_builder.py    # DAG playbook generator
â”‚
â”œâ”€â”€ phase2_engine/             # Playbook execution engine
â”‚   â”œâ”€â”€ core/                  # Core execution logic
â”‚   â””â”€â”€ playbooks/             # OWASP category playbooks (YAML)
â”‚
â”œâ”€â”€ tests/                     # Test suite
â”‚   â”œâ”€â”€ accuracy/              # Accuracy test cases
â”‚   â”œâ”€â”€ test_human_multiturn_single.py  # 50 hard test cases
â”‚   â””â”€â”€ test_phase2_multi_playbooks.py  # Multi-playbook tests
â”‚
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ test_baseline_comparison.py     # Baseline model comparison
â”‚   â”œâ”€â”€ visualize_accuracy_results.py  # Generate IEEE charts
â”‚   â””â”€â”€ generate_ieee_baseline_report.py # IEEE report generator
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ QUICKSTART.md         # Getting started guide
â”‚   â”œâ”€â”€ ARCHITECTURE.md       # System design
â”‚   â”œâ”€â”€ BASELINE_COMPARISON_GUIDE.md # Baseline testing guide
â”‚   â””â”€â”€ CLAUDE_SETUP.md       # Claude API setup
â”‚
â”œâ”€â”€ reports/                   # Test results & reports
â”‚   â”œâ”€â”€ accuracy_results_all_50_*.json # Test results
â”‚   â”œâ”€â”€ IEEE_Test_Results_Table.md     # IEEE-formatted results
â”‚   â”œâ”€â”€ accuracy_by_category_ieee.png  # Category chart (IEEE)
â”‚   â””â”€â”€ overall_accuracy_gauge_ieee.png # Accuracy gauge (IEEE)
â”‚
â””â”€â”€ test_scripts/             # Development test files
```

## ğŸš€ Quick Start

1. **Install dependencies:**
   ```powershell
   pip install -r requirements.txt
   ```

2. **Configure API key:**
   - Create `.env` file in project root
   - Add: `GEMINI_API_KEY=your-api-key-here`
   - Or set environment variable: `$env:GEMINI_API_KEY = "your-api-key-here"`
   - Optional: Add `ANTHROPIC_API_KEY` for Claude baseline comparison

3. **Run the app:**
   ```powershell
   streamlit run app.py
   ```

4. **Run tests:**
   ```powershell
   pytest tests/ -v
   ```

5. **Run baseline comparison:**
   ```powershell
   python scripts/test_baseline_comparison.py --limit 50
   ```

## ğŸ“Š Documentation

- **Getting Started**: `docs/QUICKSTART.md`
- **Architecture**: `docs/ARCHITECTURE.md`
- **Test Results**: `reports/IEEE_Test_Results_Table.md`
- **How It Works**: `docs/HOW_IT_WORK.md`
- **Baseline Comparison**: `docs/BASELINE_COMPARISON_GUIDE.md`

## ğŸ”¬ Test Results

### Latest Results (2025-11-18)
- **Overall Accuracy:** 98.0% (49/50 test cases)
- **A01 - Broken Access Control:** 92.3% (12/13)
- **A04 - Cryptographic Failures:** 100.0% (12/12) âœ…
- **A05 - Injection:** 100.0% (13/13) âœ…
- **A07 - Authentication Failures:** 100.0% (12/12) âœ…

### Test Reports
All test reports and results are in the `reports/` folder:
- `accuracy_results_all_50_20251118_152137.json` - Latest test results
- `IEEE_Test_Results_Table.md` - IEEE-formatted results
- `accuracy_by_category_ieee.png` - Category accuracy chart (IEEE format)
- `overall_accuracy_gauge_ieee.png` - Overall accuracy gauge (IEEE format)
- `results_single.csv` - Complete test results

## ğŸ› ï¸ Technology Stack

- **LLM**: Google Gemini 2.5 Pro (primary), Claude 3.5 Sonnet (baseline), OpenAI GPT-4o (baseline)
- **Web Framework**: Streamlit 1.x
- **Testing**: Pytest
- **DAG Processing**: NetworkX 3.0+
- **Visualization**: Matplotlib (IEEE-format charts)
- **Python**: 3.12.4

## ğŸ“Š Features

- âœ… **Multi-LLM Support**: Gemini, OpenAI, Claude
- âœ… **Baseline Comparison**: Compare models on same test cases
- âœ… **IEEE Visualizations**: Publication-ready charts
- âœ… **Hybrid Classification**: Rule-based + LLM + Canonical mapping
- âœ… **OWASP Top 10:2025**: Full category support
- âœ… **Automated Playbooks**: DAG-based response generation

## ğŸ”— Repository

GitHub: https://github.com/Satayu47/IncidentResponse_NEW

## ğŸ“ License

This project is part of academic research on automated incident response systems.

## ğŸ¤ Contributing

This is a research project. For questions or contributions, please open an issue on GitHub.
